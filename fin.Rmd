---
title: 'BA810 Team 7: Acovado Price Modeling'
author: "Ziqin Ma, Qiaoling Huang, Shihan Li, Chenran Peng, Elmira Ushirova"
date: "October 16, 2019"
output:
  pdf_document: default
  html_notebook: default
---

----------

## Setting ##

```{r}
options(stringsAsFactors = FALSE)
library(tidyverse)
library(lubridate)
library(ggplot2)
library(ggthemes)
library(fastDummies)
library(scales)
library(glmnet)
library(zoo)
library(ISLR)
library(leaps)
library(rpart)
library(rpart.plot)
library(tree)
library(randomForest)
library(gbm)
# Set the ggtheme beforehead for all plots
theme_set(theme_clean())
```

-----------

## Introduction ##

This notebook is created by Cohort A Team 7 for the BA 810 team project. The content includes our business setting to a existing dataset and our attempts in machine learning model training.

### Business Problem ###

Our team is planning to open a store called "Everything Avocado", basically selling avocado food products including avocado toasts, tortilla chips with guacamole, and avocado smoothies. During opening preparation, we want to set a budget in buying raw avocados as inventories, so we would like to learn the future price for avocados.

Fortunately, all of the team members are taking a machine learning course together, then we decided to develope a model to **predict avocado prices** using existing data source.

### Audience of our model ###

We expect that our optimal model could be applied to other raw product retails, especially in fresh products (fruits and vegetables). Ideally, the price any products of this type could be predicted, if the prices are influenced by similar predictors. In a business setting, whoever wants to get a sense of future product price (retailers, customers, market analysts, etc.) will hold a huge interest in our model.

--------------

## Dataset Debrief ##

* ### Data Source ###

The dataset we keep using is the `Avocado Price` dataset from [Kaggle](https://www.kaggle.com/neuromusic/avocado-prices). It is the record of historical data on avocado prices and sales volume in multiple US markets. 
```{r}
ds <- read_csv("avocado.csv")  
glimpse(ds)
```
Here is a quick view of columns and data types. There are 14 columns in this dataset, and we will use `AveragePrice` as our prediction. `X1` gives a id for each observation, and we want to transfer that into row numbers for each row by adding 1. Most of the data types are `<dbl>`, and there are only 2 columns with character vectors, `type` and `region`. We will transfer them to dummy variables to train our models. The time span is from Janurary 4, 2015 to March 25, 2018. 

* ### Descriptive Analysis ###

Before going into the modeling, we made a plot for the average price of avocados grouped by date, and we observed that average price usually increases during the second half of the year.
```{r}
ds %>% 
group_by(Date) %>%
  summarise(meanpriced = mean(AveragePrice)) %>%
  ggplot(aes(x = Date, y = meanpriced)) +
  geom_point(col = "#4a7337") +
  geom_smooth(col = "#d9cd65", se = FALSE) + 
  labs(y = "Average price",
       title = "Average price by date") + 
  theme_bw()
```

We also map the average quantity (Volume) by dates. We noticed that in the beginning of each year, there is one day when the volume sold is extremely high. We then checked the calendar and happened to find that these days were all Super Bowl game days!
```{r}
ds %>%
  group_by(Date) %>%
  summarise(totalvold = sum(`Total Volume`)) %>%
  ggplot(aes(x = Date, y = totalvold)) +
  geom_point(col = "#4a7337") +
  geom_smooth(col = "#d9cd65", se = FALSE) + 
  labs(y = "Total volumn sold",
       title = "Total volumn sold by date") + 
  theme_bw() 
```

----------

## Data Preparation ##

In the following code chunk, we took multiple steps to arrange the columns.
```{r}
# Switch the order of rows
ds <- ds %>% 
  group_by(type, region) %>% 
  select(X1, year, Date, type, region, everything()) %>% 
  arrange(Date)

# Change `X1` to `ID`
colName <- names(ds)
colName[1] <- "ID"
names(ds) <- colName

# Assign distinct number id to each observation in `ID` column
ds$ID <- seq(nrow(ds))

# Extract month from `Date` column
ds$month <- month(ds$Date)
ds <- ds %>% 
  select(ID, year, month, everything())

```


There are only 2 types of avocados recorded, conventional and organic. We decided to separate `type` column into `type_conventional` and `type_organic`.
```{r}
dsNew <- dummy_cols(ds, select_columns = "type") %>% 
  select(ID, year, month, region, type_conventional, type_organic, 
         everything(), -type)

```

The Product Look Up (PLU) codes are used by grocery retailers to make differnet product inventories. This dataset includes 3 PLU avocados, and we added a new column `other_PLU` to see the volume sold in products without PLU. The number is calculated by substracting PLU volumes from total volumes.
```{r}
dsNew$other_PLU <- dsNew$`Total Volume` - dsNew$`4046` - dsNew$`4225` - dsNew$`4770`

# Reorder the columes
dsNew <- dsNew %>% 
  select(1:3, Date, everything())
```
We initially included `other_PLU` as a predictor to our models, but then we found that `other_PLU` is eauql to `Total Bags`, and `Total Bags` is the sum of other bag types. So we will not use `other_PLU` and `Total Bags` as predictors.


Categorize `region` into US Areas
```{r}
uniqueRegion <- unique(dsNew$region)
uniqueRegion <- as.data.frame(uniqueRegion)
uniqueRegion$Area <- NA
uniqueRegion$Area[1] <- "NewEngland"
uniqueRegion$Area[2] <- "Southeast"
uniqueRegion$Area[3] <- "Mideast"
uniqueRegion$Area[4] <- "RockyMountain"
uniqueRegion$Area[5] <- "NewEngland"
uniqueRegion$Area[6] <- "Mideast"
uniqueRegion$Area[7] <- "FarWest"
uniqueRegion$Area[8] <- "Southeast"
uniqueRegion$Area[9] <- "GreatLakes"
uniqueRegion$Area[10] <- "GreatLakes"
uniqueRegion$Area[11] <- "GreatLakes"
uniqueRegion$Area[12] <- "Southwest"
uniqueRegion$Area[13] <- "RockyMountain"
uniqueRegion$Area[14] <- "GreatLakes"
uniqueRegion$Area[15] <- "GreatLakes"
uniqueRegion$Area[16] <- "GreatLakes"
uniqueRegion$Area[17] <- "Mideast"
uniqueRegion$Area[18] <- "NewEngland"
uniqueRegion$Area[19] <- "Southeast"
uniqueRegion$Area[20] <- "GreatLakes"
uniqueRegion$Area[21] <- "Southeast"
uniqueRegion$Area[22] <- "FarWest"
uniqueRegion$Area[23] <- "FarWest"
uniqueRegion$Area[24] <- "Southeast"
uniqueRegion$Area[25] <- "Southeast"
uniqueRegion$Area[26] <- "Southeast"
uniqueRegion$Area[27] <- "Southeast"
uniqueRegion$Area[28] <- "Southeast"
uniqueRegion$Area[29] <- "Mideast"
uniqueRegion$Area[30] <- "NewEngland"
uniqueRegion$Area[31] <- "NewEngland"
uniqueRegion$Area[32] <- "Southeast"
uniqueRegion$Area[33] <- "Mideast"
uniqueRegion$Area[34] <- "Southwest"
uniqueRegion$Area[35] <- "Mideast"
uniqueRegion$Area[36] <- "Plains"
uniqueRegion$Area[37] <- "FarWest"
uniqueRegion$Area[38] <- "Southeast"
uniqueRegion$Area[39] <- "Southeast"
uniqueRegion$Area[40] <- "Southeast"
uniqueRegion$Area[41] <- "FarWest"
uniqueRegion$Area[42] <- "FarWest"
uniqueRegion$Area[43] <- "FarWest"
uniqueRegion$Area[44] <- "FarWest"
uniqueRegion$Area[45] <- "Southeast"
uniqueRegion$Area[46] <- "Southeast"
uniqueRegion$Area[47] <- "Southeast"
uniqueRegion$Area[48] <- "FarWest"
uniqueRegion$Area[49] <- "Plains"
uniqueRegion$Area[50] <- "Mideast"
uniqueRegion$Area[51] <- "Southeast"
uniqueRegion$Area[52] <- "TotalUS"
uniqueRegion$Area[53] <- "FarWest"
uniqueRegion$Area[54] <- "Southwest"
names(uniqueRegion)[1] <- "region"

avo <- dsNew %>% 
  left_join(uniqueRegion, by = "region") %>% 
  select(1:5, Area, everything())


avo <- dummy_cols(avo, select_columns = "Area")

```


Rename Column Names
```{r}
colnames(avo)

names(avo)[10] <- "TotalVolume"
names(avo)[14] <- "TotalBags"
names(avo)[15] <- "SmallBags"
names(avo)[16] <- "LargeBags"
names(avo)[17] <- "XLargeBags"
names(avo)[11] <- "PLU4046"
names(avo)[12] <- "PLU4225"
names(avo)[13] <- "PLU4770"
```

----------

## Create Train and Test Datasets ##

```{r}
set.seed(1234)
avo_train <- avo %>% filter(as.Date(Date) < "2017-03-01")
avo_train %>%
  filter(year == 2017, month == 2)
avo_test <- avo %>% filter(as.Date(Date) >= "2017-03-01")
avo_test %>%
  filter(year == 2018, month == 3)


```


Base model
```{r}
f1 <- as.formula(AveragePrice ~ month+type_conventional + type_organic + TotalVolume + 
                   PLU4046 + PLU4770 + PLU4225 + SmallBags + LargeBags + XLargeBags + 
                   + Area_NewEngland + Area_Southeast
                 + Area_Mideast + Area_RockyMountain
                 + Area_FarWest + Area_GreatLakes
                 + Area_Southwest + Area_Plains + Area_TotalUS)

x1_train <- model.matrix(f1,avo_train)[,-1]
y1_train <- avo_train$AveragePrice
x1_test <- model.matrix(f1,avo_test)[,-1]
y1_test <- avo_test$AveragePrice
x1_avo <- model.matrix(f1, avo)[,-1]
```

----------

## Modeling ##

* ### Linear Regression ###

```{r}
fit.lm <- lm(f1, avo_train)
fit.lm

y.train <- avo_train$AveragePrice
y.test <- avo_test$AveragePrice
yhat.train.lm <- predict(fit.lm)
mse.train.lm <- mean((y.train - yhat.train.lm)^2)
yhat.test.lm <- predict(fit.lm, avo_test)
mse.test.lm <- mean((y.test - yhat.test.lm)^2)
mse.train.lm
mse.test.lm

coef(fit.lm)
plot(fit.lm)
```

* ### Forward Selection ###

```{r}
avo1 <- avo
avo1$ID <- NULL
avo1$year<-NULL
avo1$Date<-NULL
avo1$region<-NULL
avo1$Area<-NULL
avo1$AveragePrice<-NULL
xnames <- colnames(avo1)
xnames <- xnames[!xnames %in% c("type_conventional","type_organic", "TotalVolume", "PLU4046", "PLU4770", "PLU4225","SmallBags", "LargeBags","XLargeBags","Area_NewEngland","Area_Southeast","Area_Mideast","Area_RockyMountain","Area_FarWest","Area_GreatLakes","Area_Southwest","Area_Plains + Area_TotalUS")]
fit_fw <- lm(AveragePrice ~ 1, data = avo_train)
yhat_train <- predict(fit_fw, avo_train)
mse_train <- mean((avo_train$AveragePrice - yhat_train) ^ 2)
yhat_test <- predict(fit_fw, avo_test)
mse_test <- mean((avo_test$AveragePrice - yhat_test) ^ 2)

log_fw <-
tibble(
xname = "intercept",
model = deparse(fit_fw$call),
mse_train = mse_train,
mse_test = mse_test
)

best_mse_train <- NA
best_mse_test <- NA
best_fit_fw <- NA
best_xname <- NA
for (xname in xnames) {
# take a moment to examine and understand the following line
fit_fw_tmp <- update(fit_fw, as.formula(paste0(" ~ ", xname)))
# compute MSE train
yhat_train_tmp <- predict(fit_fw_tmp, avo_train)
mse_train_tmp <- mean((avo_train$AveragePrice - yhat_train_tmp) ^ 2)
# compute MSE test
yhat_test_tmp <- predict(fit_fw_tmp, avo_test)
mse_test_tmp <- mean((avo_test$AveragePrice - yhat_test_tmp) ^ 2)
# if this is the first predictor to be examined,
# or if this predictors yields a lower MSE that the current
# best, then store this predictor as the current best predictor
if (is.na(best_mse_test) | mse_test_tmp < best_mse_test) {
best_xname <- xname
best_fit_fw <- fit_fw_tmp
best_mse_train <- mse_train_tmp
best_mse_test <- mse_test_tmp
}
}
best_mse_train
best_mse_test
```

```{r}
log_fw <-
log_fw %>% add_row(
xname = best_xname,
model = paste0(deparse(best_fit_fw$call), collapse = ""),
mse_train = best_mse_train,
mse_test = best_mse_test
)

log_fw

# here is a complete solution
xnames<- colnames(avo1)
xnames <- xnames[!xnames %in% c("type_conventional","type_organic", "TotalVolume", "PLU4046", "PLU4770", "PLU4225","SmallBags", "LargeBags","XLargeBags","Area_NewEngland","Area_Southeast","Area_Mideast","Area_RockyMountain","Area_FarWest","Area_GreatLakes","Area_Southwest","Area_Plains + Area_TotalUS")]
fit_fw <- lm(AveragePrice ~ 1, data = avo_train)
yhat_train <- predict(fit_fw, avo_train)
yhat_test <- predict(fit_fw, avo_test)
mse_train <- mean((avo_train$AveragePrice - yhat_train)^2)
mse_test <- mean((avo_test$AveragePrice - yhat_test)^2)
xname <- "intercept"

log_fw <-
tibble(
xname = xname,
model = paste0(deparse(fit_fw$call), collapse = ""),
mse_train = mse_train,
mse_test = mse_test
)
while (length(xnames) > 0) {
best_mse_train <- NA
best_mse_test <- NA
best_fit_fw <- NA
best_xname <- NA
# select the next best predictor
for (xname in xnames) {
# take a moment to examine and understand the following line
fit_fw_tmp <- update(fit_fw, as.formula(paste0(". ~ . + ", xname)))
# compute MSE train
yhat_train_tmp <- predict(fit_fw_tmp, avo_train)
mse_train_tmp <- mean((avo_train$AveragePrice - yhat_train_tmp) ^ 2)
# compute MSE test
yhat_test_tmp <- predict(fit_fw_tmp, avo_test)
mse_test_tmp <- mean((avo_test$AveragePrice - yhat_test_tmp) ^ 2)

# if this is the first predictor to be examined,
# or if this predictors yields a lower MSE that the current
# best, then store this predictor as the current best predictor
if (is.na(best_mse_test) | mse_test_tmp < best_mse_test) {
best_xname <- xname
best_fit_fw <- fit_fw_tmp
best_mse_train <- mse_train_tmp
best_mse_test <- mse_test_tmp
}
}
log_fw <-
log_fw %>% add_row(
xname = best_xname,
model = paste0(deparse(best_fit_fw$call), collapse = ""),
mse_train = best_mse_train,
mse_test = best_mse_test
)
# adopt the best model for the next iteration
fit_fw <- best_fit_fw
# remove the current best predictor from the list of predictors
xnames <- xnames[xnames!=best_xname]
}
```

```{r}
ggplot(log_fw, aes(seq_along(xname), mse_test)) +
geom_point() +
geom_line() +
geom_point(aes(y=mse_train), color="#AA471F") +
geom_line(aes(y=mse_train), color="#AA471F") +
scale_x_continuous("Variables", labels = log_fw$xname, breaks = seq_along(log_fw$xname)) +
scale_y_continuous("MSE test") +
theme(axis.text.x = element_text(angle = 45, hjust = 1))
```

```{r}
fw<- as.formula(AveragePrice ~ month + TotalBags + Area_TotalUS + Area_Plains + other_PLU)
fit.fw<-lm(fw, data = avo_train)
```

```{r}
## calculate the yhat price for the avo dataset
yhat_avo_fw <- predict(fit.fw, avo)
df1_fw<-avo %>% 
  select(Date, AveragePrice)
df2_fw<-cbind(df1_fw, yhat_avo_fw)
colnames(df2_fw)
names(df2_fw)[3]<-"AveragePrice_hat"
## Plot the actual average price and the predictive average price
plot1_fw <- df2_fw %>% 
  group_by(Date) %>% 
  summarize(
    MeanAvg=mean(AveragePrice),
    MeanAvg_hat=mean(AveragePrice_hat))%>% 
    ggplot()+
    geom_line(aes(Date, MeanAvg),color = "#356211")+
    geom_line(aes(Date, MeanAvg_hat), color = "#cda989")+
    theme_classic()

plot1_fw + geom_vline(aes(xintercept = as.numeric(Date[113])), 
                   linetype = "dashed", size = 1,
                   color = "#AA471F")

```


* ### Backward Selection ###
Using `f1` formula to match average price with all predictors in order to see the best model which contains a giving number of predictors. 
```{r}
regfit.bwd = regsubsets(f1, data = avo_train, nvmax = 19, method = "backward")
summary(regfit.bwd)
```
Calculate the yhat and MSE for train and test dataset, the test MSE is way to large for the model including all the predictors.
```{r}
sub_model<-lm(f1, data = avo_train)
yhat_train_stepwise <- predict(sub_model, avo_train)
MSE_train_stepwise <- mean((avo_train$AveragePrice - yhat_train_stepwise)^2)
MSE_train_stepwise
yhat_test_stepwise <- predict(sub_model, avo_test)
MSE_test_stepwise <- mean((avo_test$AveragePrice - yhat_test_stepwise)^2)
MSE_test_stepwise
```
Based on the result of summary, the best fit model contains everything except  "Area_TotalUS", "type_organic". So we excluded these two predictors to create a new formula to train the model. 
```{r}
f1_1 <- as.formula(AveragePrice ~ month + type_conventional + TotalVolume + 
                      PLU4046 + PLU4770 + PLU4225 + SmallBags + LargeBags + XLargeBags + 
                      + Area_NewEngland + Area_Southeast
                    + Area_Mideast + Area_RockyMountain
                    + Area_FarWest + Area_GreatLakes
                    + Area_Southwest
                    + Area_Plains)
regfit.bwd1 = regsubsets(f1_1, data = avo_train, nvmax = 19, method = "backward")
summary(regfit.bwd1)

sub_model1<-lm(f1_1, data = avo_train)
yhat_train_stepwise1 <- predict(sub_model1, avo_train)
MSE_train_stepwise1 <- mean((avo_train$AveragePrice - yhat_train_stepwise1)^2)
MSE_train_stepwise1
yhat_test_stepwise1 <- predict(sub_model1, avo_test)
MSE_test_stepwise1 <- mean((avo_test$AveragePrice - yhat_test_stepwise1)^2)
MSE_test_stepwise1
```
However, the test MSE is still way to large, therefore, we decide to remove the predictor one by one based on the result of summary. We try to remove "Area_Plains", "Area_FarWest", "LargeBags" one by one, we found that removing "LargeBags" can significantly reduce the test MSE to 0.1397455 which is the lowest MSE that we have tried. So the best fit model is `f1_2`.
```{r}
f1_2 <- as.formula(AveragePrice ~ month + type_conventional + TotalVolume + 
                     PLU4046 + PLU4770 + PLU4225 + SmallBags + XLargeBags
                     + Area_NewEngland +
                   Area_Mideast + Area_RockyMountain
                   + Area_FarWest + Area_GreatLakes + Area_Southwest + Area_Plains
                   )
regfit.bwd2 = regsubsets(f1_2, data = avo_train, nvmax = 19, method = "backward")
summary(regfit.bwd2)

sub_model2<-lm(f1_2, data = avo_train)
yhat_train_stepwise2 <- predict(sub_model2, avo_train)
MSE_train_stepwise2 <- mean((avo_train$AveragePrice - yhat_train_stepwise2)^2)
MSE_train_stepwise2
yhat_test_stepwise2 <- predict(sub_model2, avo_test)
MSE_test_stepwise2 <- mean((avo_test$AveragePrice - yhat_test_stepwise2)^2)
MSE_test_stepwise2
```

Calculate the yhat price for the `avo` dataset which contain train and test dataset and merge them to the a single date frame.
```{r}
yhat_avo_avgprice <- predict(sub_model2, avo)
df1_bwd<-avo %>% 
  select(Date, AveragePrice)
df2_bwd<-cbind(df1_bwd, yhat_avo_avgprice)
colnames(df2_bwd)
names(df2_bwd)[3]<-"AveragePrice_hat"
```

Plot the actual average price and the predictive average price, the prediction for Backward selection is very similar with ridge and lasso based on the graph which will show as following. 
```{r}
plot1_bwd <- df2_bwd %>% 
  group_by(Date) %>% 
  summarize(
    MeanAvg=mean(AveragePrice),
    MeanAvg_hat=mean(AveragePrice_hat))%>% 
    ggplot()+
    geom_line(aes(Date, MeanAvg),color = "#356211")+
    geom_line(aes(Date, MeanAvg_hat), color = "#cda989")+
    theme_classic()

plot1_bwd + geom_vline(aes(xintercept = as.numeric(Date[113])), 
                   linetype = "dashed", size = 1,
                   color = "#AA471F")
```

* ### Ridge Regression ###

run ridge
```{R}
fit_ridge <- cv.glmnet(x1_train, y1_train, alpha = 0, nfolds = 100)
fit_ridge$lambda
```

Predict response
```{R}
yhat_train_ridge <- predict(fit_ridge, x1_train, s = fit_ridge$lambda)
yhat_test_ridge <- predict(fit_ridge, x1_test, s = fit_ridge$lambda)

mse_train_ridge = vector()
mse_test_ridge = vector()
mse_train_ridge <- mean((y1_train - yhat_train_ridge)^2)
mse_test_ridge <-mean((y1_test - yhat_test_ridge)^2)
for (i in 1:length(fit_ridge$lambda)) {
  mse_train_ridge[i] <- mean((y1_train - yhat_train_ridge)[,i]^2)
  mse_test_ridge[i] <- mean((y1_test - yhat_test_ridge)[,i]^2)
}
mse_train_ridge
mse_test_ridge
min(mse_train_ridge)
min(mse_test_ridge)

```

```{R}
lambda_min_mse_train_ridge<- fit_ridge$lambda[which.min(mse_train_ridge)]
lambda_min_mse_test_ridge <-fit_ridge$lambda[which.min(mse_test_ridge)]
lambda_min_mse_train_ridge
lambda_min_mse_test_ridge

yhat_train_ridge <- predict(fit_ridge, x1_train, s = 0.0261992)
yhat_test_ridge <- predict(fit_ridge, x1_test, s =   0.1160787)

yhat_1<- predict(fit_ridge, x1_avo, s = lambda_min_mse_test_ridge)
```

Aggregate data into one dataframe
```{R}
p1<-avo %>%
  select(Date, AveragePrice)
p2<-cbind(p1, yhat_1)
```

Plot
```{R}
p2 %>%
  group_by(Date)%>%
  summarise(meanpriced = mean(AveragePrice))

names(p2)[3] <- "pre"
p2 %>%
  group_by(Date) %>%
  summarise(meanpriced = mean(AveragePrice),meanpre = mean(pre))%>%
  ggplot()+
  geom_line(mapping = aes(x=Date,
                           y=meanpriced), col = "#356211")+
  geom_line(mapping = aes(x=Date, y= meanpre), col = "#cda989")+
  geom_vline(xintercept=as.numeric(as.Date("2017-03-01")), col = "#AA471F", linetype = "dashed") + 
  labs(title = "Ridge Regression",
       y = "Mean Price")
```
The ridge model is pretty much the same with lasso model, so we will see the difference between two models to see which model gives a more accurate prediction for our dataset after building a lasso model.


* ### Lasso Regression ###

Run lasso model
```{r}
lmodel <- glmnet(x1_train, y1_train, alpha = 1, nlambda = 100)
lmodel$lambda
```
Predict response shows the minimue mse for the train(0.06472616) and test(0.1390842). 
```{r}
y1_train_hat <- predict(lmodel, s = lmodel$lambda, newx = x1_train)
y1_test_hat <- predict(lmodel, s = lmodel$lambda, newx = x1_test)

mse_train = vector()
mse_test = vector()

for (i in 1:length(lmodel$lambda)) {
  mse_train[i] <- mean((y1_train - y1_train_hat)[,i]^2)
  mse_test[i] <- mean((y1_test - y1_test_hat)[,i]^2)
}

min(mse_train)
min(mse_test)
```
Check the minimun lambda for the train and test dataset
```{r}
lambda_min_mse_train_lasso<- lmodel$lambda[which.min(mse_train)]
lambda_min_mse_test_lasso <-lmodel$lambda[which.min(mse_test)]
lambda_min_mse_train_lasso
lambda_min_mse_test_lasso
```
Using Cross-validation fucntion to find the best lambda, the result shows the best lambda for train dataset is same as above. 
```{r}
set.seed(1)
cv.out = cv.glmnet(x1_train, y1_train, alpha = 1)
## plot the lambda
plot(cv.out)
##check the best lambda
bestlam = cv.out$lambda.min
bestlam ## the best lamdba for training dataset is same as lambda_min_mse_train
```
Check the coefficient for f1 model, the result shows that there are some predictors might be eliminated. "TotalVolume", "PLU4225", SmallBags", and "Area_TotalUS". 
```{r}
f1coef<-coef(lmodel, s = lambda_min_mse_test_lasso)
f1coef
```

Create a new formula for new predictors(eliminate the uncorrelated predictors)
```{r}
f2 <- as.formula(AveragePrice ~ month + type_conventional + type_organic + 
                   PLU4046 + PLU4770 + LargeBags + XLargeBags + 
                   Area_NewEngland + Area_Southeast + Area_Mideast + 
                   Area_RockyMountain + Area_GreatLakes + 
                   Area_Southwest + Area_Plains)
x2_train <- model.matrix(f2,avo_train)[,-1]
x2_test <- model.matrix(f2, avo_test)[,-1]
```
Run lasso model again with new predictors
```{r}
lmodel2 <- glmnet(x2_train, y1_train, alpha = 1, nlambda = 100)
```
Predict response with new predictors
```{r}
y2_train_hat <- predict(lmodel2, s = lmodel2$lambda, newx = x2_train)
y2_test_hat <- predict(lmodel2, s = lmodel2$lambda, newx = x2_test)
```
Compute MES again with new predictors, the results shows the taining data MSE is increased when eliminate the uncorrelated predictors, and the train MSE (0.06482449) increases a little but the test MSE reduces a little bit (0.1390709).
```{r}
mse_train1 = vector()
mse_test1 = vector()

for (i in 1:length(lmodel2$lambda)) {
  mse_train1[i] <- mean((y1_train - y2_train_hat)[,i]^2)
  mse_test1[i] <- mean((y1_test - y2_test_hat)[,i]^2)
}

min(mse_train1)
min(mse_test1)
```
Check again with the new minimum lambda for the train and test dataset
```{r}
lambda_min_mse_train1<- lmodel2$lambda[which.min(mse_train1)]
lambda_min_mse_test1 <-lmodel2$lambda[which.min(mse_test1)]
lambda_min_mse_train1
lambda_min_mse_test1
```
Check coefficients for f2, there is no more uncorrelated predictors in our model. 
```{r}
f2coef<-coef(lmodel2, s = lambda_min_mse_test1)
f2coef
```
Since the second model have the lowest test MSE, we decided to use "lmodel2" to make the final prediction. 
```{r}
x2_avo <- model.matrix(f2, avo)[,-1]
y2_avo_min_lambda_hat <- predict(lmodel2, s = lambda_min_mse_test1, newx = x2_avo)
class(y2_avo_min_lambda_hat)
```
Aggregate data into one dataframe for the first model prediction
```{r}
df1<-avo %>% 
  select(Date, AveragePrice)
df2<-cbind(df1, y2_avo_min_lambda_hat)

names(df2)[3]<-"AveragePrice_hat"
```
Plot the actual average price and the predictive average price
```{r}
head(df2)
class(df2$Date)
plot1 <- df2 %>% 
  group_by(Date) %>% 
  summarize(
    MeanAvg=mean(AveragePrice),
    MeanAvg_hat=mean(AveragePrice_hat)) %>%
  ggplot()+
  geom_line(aes(Date, MeanAvg),color = "#356211")+
  geom_line(aes(Date, MeanAvg_hat), color = "#cda989")+
  theme_classic()

plot1 + geom_vline(aes(xintercept = as.numeric(Date[113])), 
                   linetype = "dashed", size = 1,
                   color = "#AA471F")
```
We can see that our ridge model actually did a better job than lasso. 
Ridge is more flexibly than lasso, maybe each variable have something to do in our data set, so ridge works better than lasso. 

* ### Decision Tree ###

```{r}
fit.tree <- rpart(f1,
                  avo_train,
                  control = rpart.control(cp = 0.001))

par(xpd = TRUE)
plot(fit.tree, compress=TRUE)
text(fit.tree, use.n=TRUE)
rpart.plot(fit.tree, type = 1)
```

```{r}
yhat.train.tree <- predict(fit.tree, avo_train)
mse.train.tree <- mean((avo_train$AveragePrice - yhat.train.tree)^2)
mse.train.tree
```

```{r}
yhat.test.tree <- predict(fit.tree, avo_test)
mse.test.tree <- mean((avo_test$AveragePrice - yhat.test.tree)^2)
mse.test.tree
```

* ### Regression Tree ###

```{r}
tree.avo =tree(f1, avo_train)
summary(tree.avo)
```

```{r}
cv.avo =cv.tree(tree.avo)

prune.avo =prune.tree(tree.avo,best =5)
plot(prune.avo)
text(prune.avo,pretty =0)
```


```{r}
yhat <- predict(tree.avo, newdata = avo_test)
mse_regreTree_test <- mean((yhat - avo_test$AveragePrice)^2)
mse_regreTree_test
```


* ### Bagging ###

```{r}
set.seed (1)
bag.avo =randomForest(f1, data = avo_train, 
                           mtry = 19, importance =TRUE)
bag.avo
```

```{r}
yhat.bag = predict (bag.avo, newdata = avo_test)
plot(yhat.bag, avo_test$AveragePrice, main = "Scatter Plot for Bagging",
     xlab = "Prediction price in test set", ylab = "Average price in test set", 
     col = "#bdcc64")
abline (0,1)

mse_bag_test <- mean((yhat.bag - avo_test$AveragePrice)^2)
mse_bag_test
```


* ### Random Forests ###

Building the model first:

```{r}
fit_rf <- randomForest(f1,
                       avo_train,
                       ntree=300,
                       do.trace=F)
varImpPlot(fit_rf)
```
Predicting our y_hats for train and for test data:

```{r}
yhat_rf_train <- predict(fit_rf, avo_train)
yhat_rf_test <- predict(fit_rf, avo_test)
```

Calculating MSE for both train and test:

```{r}
mse_rf_train <- mean((yhat_rf_train - y1_train) ^ 2)
mse_rf_test <- mean((yhat_rf_test - y1_test)^2)

print(mse_rf_train)
print(mse_rf_test)
```

Plotting actual prices and predicted prices on one plot:
Have to prepare data to use it for the plot first by creating a dataframe with Date, AveragePrice and Predicted Price (y_hat) for both train and test:

```{r}
avo %>% 
  select(Date, AveragePrice) -> plot_rf_full

yhat_rf_full <- c(yhat_rf_train,yhat_rf_test )

cbind(plot_rf_full, yhat_rf_full) -> plot_rf_full1  
```



```{r}
plot_rf_full1 %>% 
  group_by(Date) %>% 
  summarise(mean_y = mean(AveragePrice),
            mean_yhat = mean(yhat_rf_full)) %>% 
  ggplot() +
  geom_line(aes(x = Date, y = mean_y), col = "#356211") +
  geom_line(aes(x = Date, y = mean_yhat), col = "#cda989") -> rf_graph
 
rf_graph + geom_vline(aes(xintercept = as.numeric(Date[113])), 
                   linetype = "dashed", size = 1,
                   color = "#AA471F") + 
  labs(title = "Random Forest",
       y = "Mean Price")
```
Conclusion for random forest: As we can see on the graph above, with 300 trees, random forest fit train data pretty well, however it is doing bad with test data. Nevertheless, the graph looks better than other less flexible models we have used.
We could have tried to increase the number of trees to see if it would give better estimations, however, unfortunately, the capacity of RStudios that we have used is not enough to run random forests with more than 300 trees.




* ### Boosting ###

```{r}
boostingcv <- gbm(f1,
                  data = avo_train,
                  distribution = "gaussian",
                  n.trees = 150,
                  interaction.depth = 4,
                  cv.folds = 10,
                  shrinkage = 0.1)
relative.influence(boostingcv)

yhat_btree <- predict(boostingcv, avo_train, n.trees = 150)
mse_btree <- mean((yhat_btree - y1_train) ^ 2)
print(mse_btree)
```

```{r}
yhat_btree_test <- predict(boostingcv, avo_test, n.trees = 150)
mse_btree_test <- mean((yhat_btree_test - y1_test) ^ 2)
print(mse_btree_test)
```

```{r}
avo_train$prediction_btree <- yhat_btree
avo_test$prediction_btree <- yhat_btree_test

avo_plot <- rbind(avo_train, avo_test)
```

```{r}
btree_plot <- avo_plot %>% 
  group_by(Date) %>% 
  summarise(meanAvg = mean(AveragePrice),
            meanAvg_hat = mean(prediction_btree)) %>% 
  ggplot() +
  geom_line(aes(Date, meanAvg), col = "#356211") + 
  geom_line(aes(Date, meanAvg_hat), col = "#cda989") + 
  labs(title = "Boosting",
       y = "Mean Price") +
  theme_clean()

btree_plot + 
  geom_vline(aes(xintercept = as.numeric(Date[113])),
             linetype = "dashed", size = 1, col = "#AA471F")
```


* ### Lasso with a New Variable ###
```{r}
ds1 <- read_csv("avocado3.csv")
glimpse(ds1)
```
The new dataset called `avocado3` is the dataset that we created based on the original avocado dataset. We create two more columns called "NewPrice" and "NewPrice2" which was based on the previous price that we have as "AveragePrice". The NewPrice was come from the AveragePrice of one row above and the "NewPrice2" was from  the AveragePrice of two rows above. We are going to add these two columns as new variables to do a more accurate prediction for our time-series based dataset. 

```{r}
ds1 <- ds1 %>% 
  group_by(type, region) %>% 
  select(X1, year, Date, type, region, everything()) %>% 
  arrange(Date)

ds1 <- ds1 %>% 
  group_by(type, region) %>% 
  select(X1, year, Date, type, region, everything()) %>% 
  arrange(Date)

colName <- names(ds1)
colName[1] <- "ID"
names(ds1) <- colName

ds1$ID <- seq(nrow(ds1))

ds1$month <- month(ds1$Date)
ds1 <- ds1 %>% 
  select(ID, year, month, everything())

ds1New <- dummy_cols(ds1, select_columns = "type") %>% 
  select(ID, year, month, region, type_conventional, type_organic, 
         everything(), -type)

ds1New$other_PLU <- ds1New$`Total Volume` - ds1New$`4046` - ds1New$`4225` - ds1New$`4770`

ds1New <- ds1New %>% 
  select(1:3, Date, everything())

uniqueRegion <- unique(ds1New$region)
uniqueRegion <- as.data.frame(uniqueRegion)
uniqueRegion$Area <- NA
uniqueRegion$Area[1] <- "NewEngland"
uniqueRegion$Area[2] <- "Southeast"
uniqueRegion$Area[3] <- "Mideast"
uniqueRegion$Area[4] <- "RockyMountain"
uniqueRegion$Area[5] <- "NewEngland"
uniqueRegion$Area[6] <- "Mideast"
uniqueRegion$Area[7] <- "FarWest"
uniqueRegion$Area[8] <- "Southeast"
uniqueRegion$Area[9] <- "GreatLakes"
uniqueRegion$Area[10] <- "GreatLakes"
uniqueRegion$Area[11] <- "GreatLakes"
uniqueRegion$Area[12] <- "Southwest"
uniqueRegion$Area[13] <- "RockyMountain"
uniqueRegion$Area[14] <- "GreatLakes"
uniqueRegion$Area[15] <- "GreatLakes"
uniqueRegion$Area[16] <- "GreatLakes"
uniqueRegion$Area[17] <- "Mideast"
uniqueRegion$Area[18] <- "NewEngland"
uniqueRegion$Area[19] <- "Southeast"
uniqueRegion$Area[20] <- "GreatLakes"
uniqueRegion$Area[21] <- "Southeast"
uniqueRegion$Area[22] <- "FarWest"
uniqueRegion$Area[23] <- "FarWest"
uniqueRegion$Area[24] <- "Southeast"
uniqueRegion$Area[25] <- "Southeast"
uniqueRegion$Area[26] <- "Southeast"
uniqueRegion$Area[27] <- "Southeast"
uniqueRegion$Area[28] <- "Southeast"
uniqueRegion$Area[29] <- "Mideast"
uniqueRegion$Area[30] <- "NewEngland"
uniqueRegion$Area[31] <- "NewEngland"
uniqueRegion$Area[32] <- "Southeast"
uniqueRegion$Area[33] <- "Mideast"
uniqueRegion$Area[34] <- "Southwest"
uniqueRegion$Area[35] <- "Mideast"
uniqueRegion$Area[36] <- "Plains"
uniqueRegion$Area[37] <- "FarWest"
uniqueRegion$Area[38] <- "Southeast"
uniqueRegion$Area[39] <- "Southeast"
uniqueRegion$Area[40] <- "Southeast"
uniqueRegion$Area[41] <- "FarWest"
uniqueRegion$Area[42] <- "FarWest"
uniqueRegion$Area[43] <- "FarWest"
uniqueRegion$Area[44] <- "FarWest"
uniqueRegion$Area[45] <- "Southeast"
uniqueRegion$Area[46] <- "Southeast"
uniqueRegion$Area[47] <- "Southeast"
uniqueRegion$Area[48] <- "FarWest"
uniqueRegion$Area[49] <- "Plains"
uniqueRegion$Area[50] <- "Mideast"
uniqueRegion$Area[51] <- "Southeast"
uniqueRegion$Area[52] <- "TotalUS"
uniqueRegion$Area[53] <- "FarWest"
uniqueRegion$Area[54] <- "Southwest"
names(uniqueRegion)[1] <- "region"

avo <- ds1New %>% 
  left_join(uniqueRegion, by = "region") %>% 
  select(1:5, Area, everything())
avo <- dummy_cols(avo, select_columns = "Area")

names(avo)[10] <- "TotalVolume"
names(avo)[14] <- "TotalBags"
names(avo)[15] <- "SmallBags"
names(avo)[16] <- "LargeBags"
names(avo)[17] <- "XLargeBags"
names(avo)[11] <- "PLU4046"
names(avo)[12] <- "PLU4225"
names(avo)[13] <- "PLU4770"

set.seed(1234)
avo_train <- avo %>% filter(as.Date(Date) < "2017-03-01")
avo_train %>%
  filter(year == 2017, month == 2)
avo_test <- avo %>% filter(as.Date(Date) >= "2017-03-01")
avo_test %>%
  filter(year == 2018, month == 3)
```

pre model
```{r}
f2 <- as.formula(AveragePrice ~ month +type_conventional + type_organic + TotalVolume + 
                   PLU4046 + PLU4770 + PLU4225 + SmallBags + LargeBags + XLargeBags + 
                   + Area_NewEngland + Area_Southeast
                 + Area_Mideast + Area_RockyMountain
                 + Area_FarWest 
                 + Area_GreatLakes + Area_Southwest
                 + Area_Plains + Area_TotalUS + NewPrice + NewPrice2)

x1_train <- model.matrix(f2,avo_train)[,-1]
y1_train <- avo_train$AveragePrice
x1_test <- model.matrix(f2, avo_test)[,-1]
y1_test <- avo_test$AveragePrice
date_test <- avo_test$Date
x1_avo <- model.matrix(f2, avo)[,-1]
```

Run lasso
```{r}
lmodel <- glmnet(x1_train, y1_train, alpha = 1, nlambda = 100)
lmodel$lambda
```

Predict response
```{r}
y1_train_hat <- predict(lmodel, s = lmodel$lambda, newx = x1_train)
y1_test_hat <- predict(lmodel, s = lmodel$lambda, newx = x1_test)
length(y1_test_hat)
mse_train = vector()
mse_test = vector()

for (i in 1:length(lmodel$lambda)) {
  mse_train[i] <- mean((y1_train - y1_train_hat)[,i]^2)
  mse_test[i] <- mean((y1_test - y1_test_hat)[,i]^2)
}
mse_train
mse_test

min(mse_test)
```

```{r}
lambda_min_mse_train<- lmodel$lambda[which.min(mse_train)]
lambda_min_mse_test <-lmodel$lambda[which.min(mse_test)]
lambda_min_mse_train
lambda_min_mse_test
```


Using Cross-validation fucntion to find the best lambda
```{r}
set.seed(1)
cv.out = cv.glmnet(x1_train, y1_train, alpha = 1)
```

Plot the lambda
```{r}
plot(cv.out,
     xlab = "Log(lambda)")

```

Check the best lambda
```{r}
bestlam = cv.out$lambda.min
bestlam ## the best lamdba for training dataset is same as lambda_min_mse_train
```

Create a new formula for new predictors(eliminate the uncorrelated predictors)
```{r}
f3 <- as.formula(AveragePrice ~ month + type_conventional + type_organic + 
                   PLU4046 + PLU4770 + PLU4225 + LargeBags + XLargeBags + 
                   Area_NewEngland + Area_Southeast + Area_Mideast + 
                   Area_RockyMountain + Area_GreatLakes + 
                   Area_Southwest + Area_Plains)
x2_train <- model.matrix(f3,avo_train)[,-1]
x2_test <- model.matrix(f3, avo_test)[,-1]
```

Run lasso model again with new predictors
```{r}
lmodel2 <- glmnet(x2_train, y1_train, alpha = 1, nlambda = 100)
```

Predict response with new predictors
```{r}
y2_train_hat <- predict(lmodel2, s = lmodel2$lambda, newx = x2_train)
y2_test_hat <- predict(lmodel2, s = lmodel2$lambda, newx = x2_test)
```

Compute MES again with new predictors. The results shows the taining data MSE is increased when eliminate the predictors which don't have correlation but the test data MSE still keep the same.
```{r}
mse_train1 = vector()
mse_test1 = vector()

for (i in 1:length(lmodel2$lambda)) {
  mse_train1[i] <- mean((y1_train - y2_train_hat)[,i]^2)
  mse_test1[i] <- mean((y1_test - y2_test_hat)[,i]^2)
}
mse_train1
mse_test1
min(mse_train1)
min(mse_test1)

lambda_min_mse_train1<- lmodel2$lambda[which.min(mse_train1)]
lambda_min_mse_test1 <-lmodel2$lambda[which.min(mse_test1)]
lambda_min_mse_train1
lambda_min_mse_test1
```

Check coefficients for f2 and f3 model
```{r}
f2coef<-coef(lmodel, s = lambda_min_mse_test)
f3coef<-coef(lmodel2, s = lambda_min_mse_test1)
f2coef 
f3coef
```

Since the second model have the training MSE increased, the ideal model is still the first "lmodel", so we decide to use "lmodel" to run the prediction
```{r}
y1_avo_min_lambda_hat <- predict(lmodel, s = lambda_min_mse_test, newx = x1_avo)
class(y1_avo_min_lambda_hat)
```

Aggregate data into one dataframe for the first model prediction
```{r}
df2<-avo %>% 
  select(Date, AveragePrice)
df3<-cbind(df2, y1_avo_min_lambda_hat)
colnames(df3)

names(df3)[3]<-"AveragePrice_hat"
```

Plot the actual average price and the predictive average price
```{r}
head(df3)
class(df3$Date)
 df3 %>% 
  group_by(Date) %>%
  summarise(meanpriced = mean(AveragePrice),meanpre = mean(AveragePrice_hat))%>%
  ggplot()+
  geom_line(mapping = aes(x=Date,
                           y=meanpriced), col = "#356211")+
  geom_line(mapping = aes(x=Date, y= meanpre), col = "#cda989")+
  geom_vline(xintercept=as.numeric(as.Date("2017-03-01")), col = "#AA471F", linetype = "dashed", size = 1)+
   labs(title = "Lasso with new variables",
        y = "Mean Price") +
   theme_classic()
```
We can see from this graph, that compare with the regular lasso that we did before, this graph did a much better prediction with our dataset. It not only showed a season based change for avocado price but also have some sort of change within one year period. 
However, Although this model have the lowest Test MSE, we can see that the prediction is not as good as some model that we have before. So it require us to work further on this time series dataset to get more accurate prediction. 



* ### Lasso with a previous day prediction as a variable ###

Because we are working with time series dataset, it is reasonable to assume that the previous day price plays a big part in predicting a next day price. 
Therefore, we have tried to create a code that would predict one day price and use it as a predictor for the next day price. 

Let's make a new dataframe with lagged 1 day price as an additional variable, and change it to zero for test data (as in real life we would not know the previous day price):
```{r}
avo_ts <- avo %>% 
          mutate(lag1_y = lag(AveragePrice))

dim(avo_ts)
avo_ts <- drop_na(avo_ts)
dim(avo_ts)

avo_train_ts <- avo_ts[1:12202, ]
avo_test_ts <- avo_ts[12203:18248, ]
dim(avo_test_ts)

avo_test_ts[2:6046,"lag1_y"] <- 0

```

Running a loop to predict each day using previous day prediction:

```{r}
f_ts <- as.formula(AveragePrice ~ lag1_y + month + type_conventional + type_organic + 
                           PLU4046 + PLU4770 + PLU4225 + LargeBags + XLargeBags + 
                           Area_NewEngland + Area_Southeast + Area_Mideast + 
                           Area_RockyMountain + Area_GreatLakes + 
                           Area_Southwest + Area_Plains)


y_hat_ts_test_f <- avo_test_ts[1,"lag1_y"]
avo_ts1 <- avo_ts
lambda_min_test <- 0.001570601 #the min lambda from lasso model above
check_ts <- vector()
for (i in 1:6045) {
  avo_train_ts <- avo_ts1[1:(12203+i), ]
  avo_test_ts <- avo_ts1[(12204+i):18248, ]
  
  x_ts_train <- model.matrix(f_ts,avo_train_ts)[,-1]
  x_ts_test <- model.matrix(f_ts, avo_test_ts)[,-1]
  
  y_ts_train <- avo_train_ts$AveragePrice
  y_ts_test <- avo_test_ts$AveragePrice
  
  lmodel_ts <- glmnet(x_ts_train, y_ts_train, alpha = 1, nlambda = 100)
  
  y_ts_train_hat <- predict(lmodel_ts, s = lambda_min_test, newx = x_ts_train)
  y_ts_test_hat <- predict(lmodel_ts, s = lambda_min_test, newx = x_ts_test)
  y_hat_ts_test_f <- c(y_hat_ts_test_f, y_ts_test_hat[1])
  (length(y_hat_ts_test_f))
  avo_ts1[(12204+i), "lag1_y"] <- y_ts_test_hat[1]
  
}
```
The loop gives back an error, which we, unfortunately, could not resolve (Also because this loop takes long time to run).

However, it somehow worked to gather prediction based on previous day prediction as variable. 
We can see it in here:
```{r}
length(y_hat_ts_test_f)
y_hat_ts_test_f <- unlist(y_hat_ts_test_f)
```

It is supposed to have 6046 numbers, but maybe due to the error it lost 2 numbers somewhere in the loop.
We suppose that 2 out of 6046 will not add a significant change, so in order to calculate MSE properly, we have decided to substitute the lost 2 numbers with an average of known 6044:

```{r}
y_hat_ts_test_f <- c(y_hat_ts_test_f, mean(y_hat_ts_test_f), mean(y_hat_ts_test_f))
length(y_hat_ts_test_f)
```

We need to also readjust test data because of the loop changes:

```{r}
avo_test_ts <- avo_ts[12203:18248,]
y_ts_test <- avo_test_ts$AveragePrice
```

We now can calculate MSE:

```{r}
mse_ts_test_f <- mean((y_ts_test - y_hat_ts_test_f)^2)
print(mse_ts_test_f)
```
The MSE shown above is the lowest MSE among all models we have run. This may implicate that for time series dataset it is better to use previous day prediction as a variable for next day prediction.
However, it is quite incovenient and time-consuming to adjust models for variables and predict each row separately. 

Let's see if plot can show the difference.

Need to select and prepare data for the plot first:
```{r}
avo %>% 
  select(Date, AveragePrice) -> plot_ts_full
plot_ts_full <- plot_ts_full[2:nrow(plot_ts_full),]

y_hat_train_ts <-  y_ts_train_hat[1:12202,]
y_hat_ts_full <- c(y_hat_train_ts, y_hat_ts_test_f )

plot_ts <- cbind(plot_ts_full, y_hat_ts_full)
```

Plot:
```{r}
plot_ts %>% 
  group_by(Date) %>% 
  summarise(mean_y = mean(AveragePrice),
            mean_yhat = mean(y_hat_ts_full)) %>% 
  ggplot() +
  geom_line(aes(x = Date, y = mean_y), col = "#356211") +
  geom_line(aes(x = Date, y = mean_yhat), col = "#cda989") -> ts_graph
 
ts_graph + geom_vline(aes(xintercept = as.numeric(Date[113])), 
                   linetype = "dashed", size = 1,
                   color = "#AA471F") + 
  labs(title = "Lasso with Loop",
       y = "Mean Price")
```
The graph does not show much of the difference, however we know that it is better because it has lower MSE.



----------

## Result ##

* ### Key to choose the best model ###

We have noticed that our dataset is in time series, and based on professor's comment, the existing models we practiced in this class might not compute the predictions as we initially expected. Since we have observed a seasonality in average prices throught our a year when we inspected the dataset, our primary rule for choosing the "best" model is to find the models with similar trend as well as the lowest test MSE compared to other models. In practice, we plot the lines for predictions and real prices for the entire dataset, and visually inspect the ones with the same trend. After we filter models by trend, we compare the test MSEs to find the "fit" model in our case.

* ### The top models ###

We have collected all test MSEs in Modeling Section. Within the scope of existing models (instructed in class), Random Forest gives the lowest test MSE (0.1279945) and the good fit in seasonality. We also created two models by adding previous predictions as variabes to predict more accurately in time-series data; together with all models we did, the lowest test MSE turns out to be 0.1113422, from the Lasso with Loop. 
Therefore, we have two candidates, Random Forest and Lasso with Loop. Random Forest stands out in existing models, while the training-in-process Lasso can give a more precious approach.

One thing we have to mention is that, in practice, none of our models could give the accuracy we expected before exploring the dataset. This is the major challenge in this project.

* ### Challenge ###

In order to catch the time influence for time-series datase, we tried two new models by adding variables from previous preictions. Specifically, Lasso with New Variables adopts old observations, and result proves that observations from previous days/periods DO influence future predictions. But this method cannot be applied in real-life practice. When adding the new columns, we lagged some rows of the test data to train the model, and this step can never be accomplished, even permitted, in actual machine learning process.

Lasso with Loop is theretically practical and applicable to deal with such data. The biggest disadvantage for this model in our training process is that it is too easy to crush the envirnment, immensely increasing our cost in modeling. If we want to play with such algorithms in real life, we will search for mature models similar to what we built here.

----------

## Conclusions ##

Given the business setting to open an "Everything Avocado" store and the goal to use models to predict future avocado price, out team found an existing time-series dataset on avocado prices and used that to try different models. After comparing the prediction trends and test MSEs, we narrow our options to Random Forest and Lasso with Loop. 

Lasso with Loop is appealing with its lowest MSE and better. However, we decide to stick on **Random Forest** as our choice of model. Random Forest can compute predictions with the lowest MSE among models we learned in class, and its outcome has the trend. On the other hand, we give up the attractive Lasso with Loop because of its high computation cost and our limitation in knowledge.

We gained a lot of insights in coding with R to train our models, converting the machine learning theories into computation. We also had an opportunity to preview the methods to analyze time-series datasets. The fundamental take-away from our new models is that, in time-series dataset, previous observation is one of the main predictors model training. Meanwhile, we also experienced the difficulties in adding time-series variables. After this project, we will keep learing time-series theories and models, and hope to train a more proper model for our avocado business.
